"""
Code Editor Service
Manages workspaces and files for the code editor
"""

import os
import uuid
from pathlib import Path
from typing import Optional, List, Dict, Any
from datetime import datetime
import logging
import json
import difflib
import hashlib

from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Request, Header
from pydantic import BaseModel, Field

from elohimos_memory import ElohimOSMemory

logger = logging.getLogger(__name__)

from fastapi import Depends
from auth_middleware import get_current_user
from permission_engine import require_perm
from audit_logger import get_audit_logger, AuditAction

router = APIRouter(
    prefix="/api/v1/code-editor",
    tags=["code-editor"],
    dependencies=[Depends(get_current_user)]  # Require auth
)

# Initialize memory system
memory = ElohimOSMemory()

# Ensure code editor tables exist
def init_code_editor_db():
    """Initialize code editor database tables"""
    conn = memory.memory.conn

    # Workspaces table
    conn.execute("""
        CREATE TABLE IF NOT EXISTS code_editor_workspaces (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            source_type TEXT NOT NULL CHECK(source_type IN ('disk', 'database')),
            disk_path TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)

    # Files table
    conn.execute("""
        CREATE TABLE IF NOT EXISTS code_editor_files (
            id TEXT PRIMARY KEY,
            workspace_id TEXT NOT NULL,
            name TEXT NOT NULL,
            path TEXT NOT NULL,
            content TEXT NOT NULL,
            language TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (workspace_id) REFERENCES code_editor_workspaces(id) ON DELETE CASCADE
        )
    """)

    conn.commit()
    logger.info("âœ“ Code editor database tables initialized")

# Initialize on import
init_code_editor_db()


# Security: Path traversal guard
def ensure_under_root(root: Path, candidate: Path) -> None:
    """
    Ensure candidate path is under root directory.
    Prevents path traversal attacks.

    Raises HTTPException(400) if validation fails.
    """
    try:
        root_resolved = root.resolve()
        candidate_resolved = candidate.resolve()

        # Check if candidate is relative to root
        if not candidate_resolved.is_relative_to(root_resolved):
            logger.warning(f"Path traversal attempt: {candidate} not under {root}")
            raise HTTPException(
                status_code=400,
                detail=f"Path must be under workspace root"
            )
    except (ValueError, OSError) as e:
        logger.error(f"Path validation error: {e}")
        raise HTTPException(
            status_code=400,
            detail="Invalid path"
        )


# Models
class WorkspaceCreate(BaseModel):
    name: str
    source_type: str  # 'disk' or 'database'
    disk_path: Optional[str] = None


class WorkspaceResponse(BaseModel):
    id: str
    name: str
    source_type: str
    disk_path: Optional[str]
    created_at: datetime
    updated_at: datetime


class FileCreate(BaseModel):
    workspace_id: str
    name: str
    path: str
    content: str
    language: str


class FileUpdate(BaseModel):
    name: Optional[str] = None
    path: Optional[str] = None
    content: Optional[str] = None
    language: Optional[str] = None
    base_updated_at: Optional[str] = Field(None, description="Base timestamp for optimistic concurrency")


class FileDiffRequest(BaseModel):
    new_content: str
    base_updated_at: Optional[str] = Field(None, description="Base timestamp to check for conflicts")


class FileDiffResponse(BaseModel):
    diff: str
    current_hash: str
    current_updated_at: str
    conflict: bool = False
    truncated: bool = False
    max_lines: Optional[int] = None
    shown_head: Optional[int] = None
    shown_tail: Optional[int] = None
    message: Optional[str] = None


class FileResponse(BaseModel):
    id: str
    workspace_id: str
    name: str
    path: str
    content: str
    language: str
    created_at: datetime
    updated_at: datetime


class FileTreeNode(BaseModel):
    id: str
    name: str
    path: str
    is_directory: bool
    children: Optional[List['FileTreeNode']] = None


# Helper functions
def build_file_tree(workspace_id: str) -> List[FileTreeNode]:
    """Build hierarchical file tree from flat file list"""
    conn = memory.memory.conn

    files = conn.execute("""
        SELECT id, name, path, content
        FROM code_editor_files
        WHERE workspace_id = ?
        ORDER BY path
    """, (workspace_id,)).fetchall()

    # Build tree structure
    root_nodes = []
    path_map = {}

    for file_row in files:
        file_id, name, path, content = file_row
        parts = path.split('/')

        # Handle root files
        if len(parts) == 1:
            root_nodes.append(FileTreeNode(
                id=file_id,
                name=name,
                path=path,
                is_directory=False
            ))
            continue

        # Build directory structure
        current_path = ""
        for i, part in enumerate(parts[:-1]):
            current_path = f"{current_path}/{part}" if current_path else part

            if current_path not in path_map:
                node = FileTreeNode(
                    id=f"dir_{current_path}",
                    name=part,
                    path=current_path,
                    is_directory=True,
                    children=[]
                )
                path_map[current_path] = node

                # Add to parent or root
                if i == 0:
                    root_nodes.append(node)
                else:
                    parent_path = "/".join(parts[:i])
                    if parent_path in path_map:
                        path_map[parent_path].children.append(node)

        # Add file to parent directory
        file_node = FileTreeNode(
            id=file_id,
            name=name,
            path=path,
            is_directory=False
        )

        parent_path = "/".join(parts[:-1])
        if parent_path in path_map:
            path_map[parent_path].children.append(file_node)

    return root_nodes


def scan_disk_directory(dir_path: str) -> List[Dict[str, Any]]:
    """Recursively scan directory and return file list"""
    files = []
    base_path = Path(dir_path)

    # Ignore patterns
    ignore_patterns = {
        '.git', 'node_modules', '__pycache__', '.venv', 'venv',
        '.DS_Store', '.vscode', '.idea', 'dist', 'build'
    }

    for file_path in base_path.rglob('*'):
        # Skip ignored directories
        if any(ignored in file_path.parts for ignored in ignore_patterns):
            continue

        # Skip directories themselves
        if file_path.is_dir():
            continue

        # Get relative path
        rel_path = file_path.relative_to(base_path)

        # Detect language from extension
        ext = file_path.suffix.lower()
        lang_map = {
            '.js': 'javascript', '.jsx': 'javascript',
            '.ts': 'typescript', '.tsx': 'typescript',
            '.py': 'python',
            '.java': 'java',
            '.cpp': 'cpp', '.cc': 'cpp', '.cxx': 'cpp',
            '.c': 'c',
            '.go': 'go',
            '.rs': 'rust',
            '.rb': 'ruby',
            '.php': 'php',
            '.html': 'html',
            '.css': 'css',
            '.json': 'json',
            '.yaml': 'yaml', '.yml': 'yaml',
            '.md': 'markdown',
            '.sql': 'sql',
            '.sh': 'shell',
        }
        language = lang_map.get(ext, 'plaintext')

        try:
            content = file_path.read_text(encoding='utf-8')
        except Exception:
            # Skip binary files or files that can't be read
            continue

        files.append({
            'name': file_path.name,
            'path': str(rel_path).replace('\\', '/'),
            'content': content,
            'language': language
        })

    return files


# Endpoints
@router.post("/workspaces", response_model=WorkspaceResponse)
@require_perm("code.edit")
async def create_workspace(request: Request, workspace: WorkspaceCreate, current_user: dict = Depends(get_current_user)):
    """Create a new database workspace"""
    try:
        if workspace.source_type != 'database':
            raise HTTPException(status_code=400, detail="Only database workspaces can be created this way")

        workspace_id = str(uuid.uuid4())
        conn = memory.memory.conn

        conn.execute("""
            INSERT INTO code_editor_workspaces (id, name, source_type)
            VALUES (?, ?, 'database')
        """, (workspace_id, workspace.name))

        conn.commit()

        # Audit log
        try:
            audit_logger = get_audit_logger()
            audit_logger.log(
                user_id=current_user.get("user_id"),
                action=AuditAction.CODE_WORKSPACE_CREATED,
                resource="code_workspace",
                resource_id=workspace_id,
                details={"name": workspace.name, "source_type": "database"}
            )
        except Exception as audit_error:
            logger.warning(f"Audit logging failed: {audit_error}")

        # Return workspace info
        created_workspace = conn.execute("""
            SELECT id, name, source_type, disk_path, created_at, updated_at
            FROM code_editor_workspaces
            WHERE id = ?
        """, (workspace_id,)).fetchone()

        return WorkspaceResponse(
            id=created_workspace[0],
            name=created_workspace[1],
            source_type=created_workspace[2],
            disk_path=created_workspace[3],
            created_at=created_workspace[4],
            updated_at=created_workspace[5]
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to create workspace: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/workspaces/open-disk", response_model=WorkspaceResponse)
@require_perm("code.edit")
async def open_disk_workspace(request: Request, name: str = Form(...), disk_path: str = Form(...), current_user: dict = Depends(get_current_user)):
    """Open folder from disk and create workspace"""
    try:
        # Validate path exists
        path = Path(disk_path)
        if not path.exists() or not path.is_dir():
            raise HTTPException(status_code=400, detail="Invalid directory path")

        # Create workspace
        workspace_id = str(uuid.uuid4())
        conn = memory.memory.conn

        conn.execute("""
            INSERT INTO code_editor_workspaces (id, name, source_type, disk_path)
            VALUES (?, ?, 'disk', ?)
        """, (workspace_id, name, str(path.absolute())))

        # Scan and import files
        files = scan_disk_directory(str(path))
        logger.info(f"Found {len(files)} files in {disk_path}")

        for file_data in files:
            file_id = str(uuid.uuid4())
            conn.execute("""
                INSERT INTO code_editor_files (id, workspace_id, name, path, content, language)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                file_id,
                workspace_id,
                file_data['name'],
                file_data['path'],
                file_data['content'],
                file_data['language']
            ))

        conn.commit()

        # Audit log
        try:
            audit_logger = get_audit_logger()
            audit_logger.log(
                user_id=current_user.get("user_id"),
                action=AuditAction.CODE_WORKSPACE_CREATED,
                resource="code_workspace",
                resource_id=workspace_id,
                details={"name": name, "source_type": "disk", "disk_path": str(path.absolute()), "files_imported": len(files)}
            )
        except Exception as audit_error:
            logger.warning(f"Audit logging failed: {audit_error}")

        # Return workspace info
        workspace = conn.execute("""
            SELECT id, name, source_type, disk_path, created_at, updated_at
            FROM code_editor_workspaces
            WHERE id = ?
        """, (workspace_id,)).fetchone()

        return WorkspaceResponse(
            id=workspace[0],
            name=workspace[1],
            source_type=workspace[2],
            disk_path=workspace[3],
            created_at=workspace[4],
            updated_at=workspace[5]
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to open disk workspace: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/workspaces/open-database", response_model=WorkspaceResponse)
@require_perm("code.use")
async def open_database_workspace(request: Request, workspace_id: str = Form(...), current_user: dict = Depends(get_current_user)):
    """Open existing workspace from database"""
    try:
        conn = memory.memory.conn

        workspace = conn.execute("""
            SELECT id, name, source_type, disk_path, created_at, updated_at
            FROM code_editor_workspaces
            WHERE id = ?
        """, (workspace_id,)).fetchone()

        if not workspace:
            raise HTTPException(status_code=404, detail="Workspace not found")

        return WorkspaceResponse(
            id=workspace[0],
            name=workspace[1],
            source_type=workspace[2],
            disk_path=workspace[3],
            created_at=workspace[4],
            updated_at=workspace[5]
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/workspaces")
@require_perm("code.use")
async def list_workspaces(current_user: dict = Depends(get_current_user)):
    """Get all workspaces"""
    try:
        conn = memory.memory.conn

        workspaces = conn.execute("""
            SELECT id, name, source_type, disk_path, created_at, updated_at
            FROM code_editor_workspaces
            ORDER BY updated_at DESC
        """).fetchall()

        return {
            "workspaces": [
                {
                    "id": w[0],
                    "name": w[1],
                    "source_type": w[2],
                    "disk_path": w[3],
                    "created_at": w[4],
                    "updated_at": w[5]
                }
                for w in workspaces
            ]
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/workspaces/{workspace_id}/files")
@require_perm("code.use")
async def get_workspace_files(workspace_id: str, current_user: dict = Depends(get_current_user)):
    """Get file tree for workspace"""
    try:
        tree = build_file_tree(workspace_id)
        return {"files": [node.dict() for node in tree]}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/files/{file_id}", response_model=FileResponse)
@require_perm("code.use")
async def get_file(file_id: str, current_user: dict = Depends(get_current_user)):
    """Get file content"""
    try:
        conn = memory.memory.conn

        file = conn.execute("""
            SELECT id, workspace_id, name, path, content, language, created_at, updated_at
            FROM code_editor_files
            WHERE id = ?
        """, (file_id,)).fetchone()

        if not file:
            raise HTTPException(status_code=404, detail="File not found")

        return FileResponse(
            id=file[0],
            workspace_id=file[1],
            name=file[2],
            path=file[3],
            content=file[4],
            language=file[5],
            created_at=file[6],
            updated_at=file[7]
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# Diff size limits (configurable)
MAX_DIFF_FILE_SIZE = 10 * 1024 * 1024  # 10 MB
MAX_DIFF_LINES = 10_000  # Max lines in diff
TRUNCATE_HEAD_LINES = 200  # Head lines to show when truncated
TRUNCATE_TAIL_LINES = 200  # Tail lines to show when truncated


@router.post("/files/{file_id}/diff", response_model=FileDiffResponse)
@require_perm("code.use")
async def get_file_diff(file_id: str, diff_request: FileDiffRequest, current_user: dict = Depends(get_current_user)):
    """
    Generate unified diff between current file content and proposed new content.
    Optionally detects conflicts if base_updated_at is provided.
    Truncates large diffs with flags.
    """
    try:
        conn = memory.memory.conn

        # Get current file
        file = conn.execute("""
            SELECT content, updated_at
            FROM code_editor_files
            WHERE id = ?
        """, (file_id,)).fetchone()

        if not file:
            raise HTTPException(status_code=404, detail="File not found")

        current_content = file[0] or ""
        current_updated_at = file[1]

        # Check for conflict if base timestamp provided
        conflict = False
        if diff_request.base_updated_at and diff_request.base_updated_at != current_updated_at:
            conflict = True
            logger.warning(f"File {file_id} has been modified since base timestamp")

        # Check file size limits
        if len(current_content) > MAX_DIFF_FILE_SIZE or len(diff_request.new_content) > MAX_DIFF_FILE_SIZE:
            logger.warning(
                f"File {file_id} exceeds size limit: current={len(current_content)} bytes, new={len(diff_request.new_content)} bytes"
            )
            # Generate hash of current content
            current_hash = hashlib.sha256(current_content.encode('utf-8')).hexdigest()

            return FileDiffResponse(
                diff=f"""[Diff unavailable - file exceeds {MAX_DIFF_FILE_SIZE / 1024 / 1024:.1f}MB limit]

Current: {len(current_content):,} bytes
Proposed: {len(diff_request.new_content):,} bytes""",
                current_hash=current_hash,
                current_updated_at=current_updated_at,
                conflict=conflict,
                truncated=True,
                max_lines=0,
                shown_head=0,
                shown_tail=0,
                message="Diff unavailable for files exceeding size limit"
            )

        # Generate unified diff
        current_lines = current_content.splitlines(keepends=True)
        new_lines = diff_request.new_content.splitlines(keepends=True)

        diff = difflib.unified_diff(
            current_lines,
            new_lines,
            fromfile='current',
            tofile='proposed',
            lineterm=''
        )

        diff_lines = list(diff)

        # Check if diff exceeds line limit
        if len(diff_lines) > MAX_DIFF_LINES:
            logger.warning(f"Diff for file {file_id} exceeds {MAX_DIFF_LINES} lines, truncating")

            # Take head and tail
            head = diff_lines[:TRUNCATE_HEAD_LINES]
            tail = diff_lines[-TRUNCATE_TAIL_LINES:]

            truncated_diff = (
                '
'.join(head) +
                f"

... [Truncated: {len(diff_lines) - TRUNCATE_HEAD_LINES - TRUNCATE_TAIL_LINES:,} lines omitted] ...

" +
                '
'.join(tail)
            )

            # Generate hash of current content
            current_hash = hashlib.sha256(current_content.encode('utf-8')).hexdigest()

            return FileDiffResponse(
                diff=truncated_diff,
                current_hash=current_hash,
                current_updated_at=current_updated_at,
                conflict=conflict,
                truncated=True,
                max_lines=MAX_DIFF_LINES,
                shown_head=TRUNCATE_HEAD_LINES,
                shown_tail=TRUNCATE_TAIL_LINES,
                message=f"Diff truncated: showing first {TRUNCATE_HEAD_LINES} and last {TRUNCATE_TAIL_LINES} lines of {len(diff_lines):,} total"
            )

        diff_str = '
'.join(diff_lines)

        # Generate hash of current content
        current_hash = hashlib.sha256(current_content.encode('utf-8')).hexdigest()

        return FileDiffResponse(
            diff=diff_str,
            current_hash=current_hash,
            current_updated_at=current_updated_at,
            conflict=conflict
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to generate diff: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/files", response_model=FileResponse)
@require_perm("code.edit")
async def create_file(request: Request, file: FileCreate, current_user: dict = Depends(get_current_user)):
    """Create new file in workspace"""
    try:
        file_id = str(uuid.uuid4())
        conn = memory.memory.conn

        conn.execute("""
            INSERT INTO code_editor_files (id, workspace_id, name, path, content, language)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            file_id,
            file.workspace_id,
            file.name,
            file.path,
            file.content,
            file.language
        ))

        # Update workspace timestamp
        conn.execute("""
            UPDATE code_editor_workspaces
            SET updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """, (file.workspace_id,))

        conn.commit()

        # If disk workspace, also write to disk
        workspace = conn.execute("""
            SELECT source_type, disk_path
            FROM code_editor_workspaces
            WHERE id = ?
        """, (file.workspace_id,)).fetchone()

        if workspace and workspace[0] == 'disk' and workspace[1]:
            workspace_root = Path(workspace[1])
            disk_file_path = workspace_root / file.path

            # Path guard: ensure file is under workspace root
            ensure_under_root(workspace_root, disk_file_path)

            disk_file_path.parent.mkdir(parents=True, exist_ok=True)
            disk_file_path.write_text(file.content, encoding='utf-8')

        # Audit log
        try:
            audit_logger = get_audit_logger()
            audit_logger.log(
                user_id=current_user.get("user_id"),
                action=AuditAction.CODE_FILE_CREATED,
                resource="code_file",
                resource_id=file_id,
                details={"workspace_id": file.workspace_id, "path": file.path, "name": file.name}
            )
        except Exception as audit_error:
            logger.warning(f"Audit logging failed: {audit_error}")

        # Return created file
        created_file = conn.execute("""
            SELECT id, workspace_id, name, path, content, language, created_at, updated_at
            FROM code_editor_files
            WHERE id = ?
        """, (file_id,)).fetchone()

        return FileResponse(
            id=created_file[0],
            workspace_id=created_file[1],
            name=created_file[2],
            path=created_file[3],
            content=created_file[4],
            language=created_file[5],
            created_at=created_file[6],
            updated_at=created_file[7]
        )

    except Exception as e:
        logger.error(f"Failed to create file: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.put("/files/{file_id}", response_model=FileResponse)
@require_perm("code.edit")
async def update_file(request: Request, file_id: str, file_update: FileUpdate, current_user: dict = Depends(get_current_user)):
    """Update file"""
    try:
        conn = memory.memory.conn

        # Get current file (include updated_at for optimistic concurrency)
        current = conn.execute("""
            SELECT workspace_id, name, path, content, language, updated_at
            FROM code_editor_files
            WHERE id = ?
        """, (file_id,)).fetchone()

        if not current:
            raise HTTPException(status_code=404, detail="File not found")

        # Optimistic concurrency: check if file was modified since base timestamp
        if file_update.base_updated_at and file_update.base_updated_at != current[5]:
            raise HTTPException(
                status_code=409,
                detail={
                    "error": "Conflict: File has been modified by another user",
                    "current_updated_at": current[5],
                    "your_base_updated_at": file_update.base_updated_at
                }
            )

        # Build update
        updates = []
        values = []

        if file_update.name is not None:
            updates.append("name = ?")
            values.append(file_update.name)
        if file_update.path is not None:
            updates.append("path = ?")
            values.append(file_update.path)
        if file_update.content is not None:
            updates.append("content = ?")
            values.append(file_update.content)
        if file_update.language is not None:
            updates.append("language = ?")
            values.append(file_update.language)

        updates.append("updated_at = CURRENT_TIMESTAMP")
        values.append(file_id)

        conn.execute(f"""
            UPDATE code_editor_files
            SET {', '.join(updates)}
            WHERE id = ?
        """, values)

        # Update workspace timestamp
        conn.execute("""
            UPDATE code_editor_workspaces
            SET updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """, (current[0],))

        conn.commit()

        # If disk workspace, also write to disk
        workspace = conn.execute("""
            SELECT source_type, disk_path
            FROM code_editor_workspaces
            WHERE id = ?
        """, (current[0],)).fetchone()

        if workspace and workspace[0] == 'disk' and workspace[1]:
            workspace_root = Path(workspace[1])
            new_path = file_update.path if file_update.path is not None else current[2]
            new_content = file_update.content if file_update.content is not None else current[3]

            disk_file_path = workspace_root / new_path

            # Path guard: ensure file is under workspace root
            ensure_under_root(workspace_root, disk_file_path)

            disk_file_path.parent.mkdir(parents=True, exist_ok=True)
            disk_file_path.write_text(new_content, encoding='utf-8')

            # If path changed, delete old file
            if file_update.path is not None and file_update.path != current[2]:
                old_path = workspace_root / current[2]
                ensure_under_root(workspace_root, old_path)  # Guard old path too
                if old_path.exists():
                    old_path.unlink()

        # Audit log
        try:
            audit_logger = get_audit_logger()
            audit_logger.log(
                user_id=current_user.get("user_id"),
                action=AuditAction.CODE_FILE_UPDATED,
                resource="code_file",
                resource_id=file_id,
                details={"workspace_id": current[0], "path": new_path if file_update.path else current[2]}
            )
        except Exception as audit_error:
            logger.warning(f"Audit logging failed: {audit_error}")

        # Return updated file
        updated_file = conn.execute("""
            SELECT id, workspace_id, name, path, content, language, created_at, updated_at
            FROM code_editor_files
            WHERE id = ?
        """, (file_id,)).fetchone()

        return FileResponse(
            id=updated_file[0],
            workspace_id=updated_file[1],
            name=updated_file[2],
            path=updated_file[3],
            content=updated_file[4],
            language=updated_file[5],
            created_at=updated_file[6],
            updated_at=updated_file[7]
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update file: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/files/{file_id}")
@require_perm("code.edit")
async def delete_file(request: Request, file_id: str, current_user: dict = Depends(get_current_user)):
    """Delete file"""
    try:
        conn = memory.memory.conn

        # Get file info before deletion
        file_info = conn.execute("""
            SELECT workspace_id, path
            FROM code_editor_files
            WHERE id = ?
        """, (file_id,)).fetchone()

        if not file_info:
            raise HTTPException(status_code=404, detail="File not found")

        workspace_id, file_path = file_info

        # Delete from database
        conn.execute("DELETE FROM code_editor_files WHERE id = ?", (file_id,))

        # Update workspace timestamp
        conn.execute("""
            UPDATE code_editor_workspaces
            SET updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """, (workspace_id,))

        conn.commit()

        # If disk workspace, also delete from disk
        workspace = conn.execute("""
            SELECT source_type, disk_path
            FROM code_editor_workspaces
            WHERE id = ?
        """, (workspace_id,)).fetchone()

        if workspace and workspace[0] == 'disk' and workspace[1]:
            workspace_root = Path(workspace[1])
            disk_file_path = workspace_root / file_path

            # Path guard
            ensure_under_root(workspace_root, disk_file_path)

            if disk_file_path.exists():
                disk_file_path.unlink()

        # Audit log
        try:
            audit_logger = get_audit_logger()
            audit_logger.log(
                user_id=current_user.get("user_id"),
                action=AuditAction.CODE_FILE_DELETED,
                resource="code_file",
                resource_id=file_id,
                details={"workspace_id": workspace_id, "path": file_path}
            )
        except Exception as audit_error:
            logger.warning(f"Audit logging failed: {audit_error}")

        return {"success": True}

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/files/import")
@require_perm("code.edit")
async def import_file(
    request: Request,
    workspace_id: str = Form(...),
    file: UploadFile = File(...),
    current_user: dict = Depends(get_current_user)
):
    """Import file into workspace"""
    try:
        # Sanitize filename to prevent path traversal (HIGH-01)
        from utils import sanitize_filename
        safe_filename = sanitize_filename(file.filename or "untitled")

        # Read file content
        content = await file.read()
        content_str = content.decode('utf-8')

        # Detect language
        ext = Path(safe_filename).suffix.lower()
        lang_map = {
            '.js': 'javascript', '.ts': 'typescript', '.py': 'python',
            '.java': 'java', '.cpp': 'cpp', '.go': 'go', '.rs': 'rust',
            '.html': 'html', '.css': 'css', '.json': 'json',
            '.md': 'markdown', '.yaml': 'yaml', '.yml': 'yaml',
        }
        language = lang_map.get(ext, 'plaintext')

        # Create file
        file_create = FileCreate(
            workspace_id=workspace_id,
            name=safe_filename,
            path=safe_filename,
            content=content_str,
            language=language
        )

        result = await create_file(request, file_create, current_user)

        # Audit log for import action (in addition to create_file's audit log)
        try:
            audit_logger = get_audit_logger()
            audit_logger.log(
                user_id=current_user.get("user_id"),
                action=AuditAction.CODE_FILE_IMPORTED,
                resource="code_file",
                resource_id=result.id,
                details={"workspace_id": workspace_id, "filename": safe_filename}
            )
        except Exception as audit_error:
            logger.warning(f"Audit logging failed: {audit_error}")

        return result

    except Exception as e:
        logger.error(f"Failed to import file: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/workspaces/{workspace_id}/sync")
@require_perm("code.edit")
async def sync_workspace(request: Request, workspace_id: str, current_user: dict = Depends(get_current_user)):
    """Sync disk workspace with filesystem"""
    try:
        conn = memory.memory.conn

        workspace = conn.execute("""
            SELECT source_type, disk_path
            FROM code_editor_workspaces
            WHERE id = ?
        """, (workspace_id,)).fetchone()

        if not workspace:
            raise HTTPException(status_code=404, detail="Workspace not found")

        if workspace[0] != 'disk':
            raise HTTPException(status_code=400, detail="Only disk workspaces can be synced")

        if not workspace[1]:
            raise HTTPException(status_code=400, detail="No disk path configured")

        # Rescan directory
        files = scan_disk_directory(workspace[1])

        # Clear existing files
        conn.execute("DELETE FROM code_editor_files WHERE workspace_id = ?", (workspace_id,))

        # Re-import files
        for file_data in files:
            file_id = str(uuid.uuid4())
            conn.execute("""
                INSERT INTO code_editor_files (id, workspace_id, name, path, content, language)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                file_id,
                workspace_id,
                file_data['name'],
                file_data['path'],
                file_data['content'],
                file_data['language']
            ))

        # Update workspace timestamp
        conn.execute("""
            UPDATE code_editor_workspaces
            SET updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """, (workspace_id,))

        conn.commit()

        # Audit log
        try:
            audit_logger = get_audit_logger()
            audit_logger.log(
                user_id=current_user.get("user_id"),
                action=AuditAction.CODE_WORKSPACE_SYNCED,
                resource="code_workspace",
                resource_id=workspace_id,
                details={"files_synced": len(files), "disk_path": workspace[1]}
            )
        except Exception as audit_error:
            logger.warning(f"Audit logging failed: {audit_error}")

        return {"success": True, "files_synced": len(files)}

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
